"""
Построение карты глубины на одной камере с помощью её перемещения.
Использует оптический поток для оценки параллакса и преобразования в карту глубины.
"""

import cv2
import numpy as np
import time


class OpticalFlowDepthEstimator:
    """
    Оценщик карты глубины на основе движения одной камеры.
    
    Принцип работы:
    1. Вычисляет оптический поток между последовательными кадрами
    2. Оценивает глобальное движение камеры (эго-движение) через гомографию (RANSAC)
    3. Выделяет остаточный поток (параллакс) = общий поток - эго-движение
    4. Преобразует параллакс в глубину: чем больше параллакс → тем ближе объект
    
    Важно: класс НЕ управляет камерой! Принимает кадры извне через __call__().
    """
    
    def __init__(self, min_inliers=15, motion_timeout=1.5):
        """
        Инициализация оценщика глубины.
        
        Параметры:
        ----------
        min_inliers : int
            Минимальное количество точек-выбросов (inliers) для валидации гомографии.
            Меньше значения → чаще ложные срабатывания, больше → строже к движению.
        motion_timeout : float
            Время (в секундах), в течение которого считается, что камера "ещё движется"
            после последнего валидного обнаружения движения. Улучшает стабильность вывода.
        """
        # === ПАРАМЕТРЫ АЛГОРИТМА ===
        self.min_inliers = min_inliers          # Порог для валидации гомографии
        self.motion_timeout = motion_timeout    # Таймаут "памяти" движения камеры
        
        # === ВНУТРЕННЕЕ СОСТОЯНИЕ (stateful) ===
        # Предыдущий кадр в оттенках серого для вычисления оптического потока
        # Инициализируется при первом вызове __call__()
        self.prev_gray = None
        
        # Время последнего валидного обнаружения движения камеры
        self.last_move_time = time.time()
        
        # Флаг: двигалась ли камера в допустимом временном окне
        self.camera_moving = False
        
        # Стабилизированная карта глубины для внутреннего сглаживания (не отображается напрямую)
        self.stable_depth = None
        
        print(f"[ИНФО] Инициализирован оценщик глубины на основе оптического потока")
        print(f"       Порог inliers: {min_inliers}, Таймаут движения: {motion_timeout} сек")
    
    def __call__(self, frame):
        """
        Обработка одного кадра и возврат карты глубины.
        
        Параметры:
        ----------
        frame : np.ndarray
            Кадр в формате BGR (OpenCV), размер (H, W, 3)
        
        Возвращает:
        -----------
        depth_map : np.ndarray или None
            Карта глубины в диапазоне [0.0, 1.0] (float32) или None, если недостаточно данных
            для расчёта (первый кадр, ошибка обработки, отсутствие движения)
        """
        # === ШАГ 1: Проверка входных данных ===
        if frame is None or frame.size == 0:
            print("[ПРЕДУПРЕЖДЕНИЕ] Получен пустой кадр")
            return None
        
        # === ШАГ 2: Конвертация в оттенки серого ===
        # Оптический поток работает только с одноканальными изображениями.
        # cv2.cvtColor(src, code) преобразует цветовое пространство:
        #   cv2.COLOR_BGR2GRAY — усредняет каналы BGR в один канал яркости
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # === ШАГ 3: Инициализация при первом кадре ===
        if self.prev_gray is None:
            # Сохраняем текущий кадр как "предыдущий" для следующей итерации
            self.prev_gray = gray.copy()
            print("[ИНФО] Первый кадр получен. Ожидаем движения камеры для расчёта глубины...")
            return None  # Невозможно вычислить поток без предыдущего кадра
        
        # === ШАГ 4: Проверка совпадения размеров кадров ===
        # Размеры могут измениться при ресайзе или смене источника видео
        if gray.shape != self.prev_gray.shape:
            print(f"[ПРЕДУПРЕЖДЕНИЕ] Изменение размера кадра: {self.prev_gray.shape} → {gray.shape}")
            # Сбрасываем состояние — начинаем заново с нового кадра
            self.prev_gray = gray.copy()
            self.stable_depth = None
            return None
        
        # === ШАГ 5: Проверка состояния движения камеры ===
        # Обновляем флаг движения на основе таймаута
        current_time = time.time()
        self.camera_moving = (current_time - self.last_move_time) < self.motion_timeout
        
        # === ШАГ 6: Вычисление плотного оптического потока (алгоритм Farneback) ===
        # cv2.calcOpticalFlowFarneback() вычисляет вектор движения для КАЖДОГО пикселя.
        # Параметры:
        #   prev: предыдущий кадр (оттенки серого)
        #   next: текущий кадр (оттенки серого)
        #   flow: None — результат будет возвращён, а не записан в существующий массив
        #   pyr_scale=0.5: масштабирование пирамиды (каждый уровень в 2 раза меньше предыдущего)
        #   levels=3: количество уровней пирамиды (больше → точнее, но медленнее)
        #   winsize=15: размер окна для полиномиальной аппроксимации (больше → сглаженнее поток)
        #   iterations=3: количество итераций на каждом уровне пирамиды
        #   poly_n=5: размер окна для вычисления производных (5 или 7)
        #   poly_sigma=1.2: стандартное отклонение гауссиана для производных
        #   flags=0: дополнительные флаги (0 — стандартный режим)
        #
        # Результат: flow.shape = (H, W, 2)
        #   flow[y, x, 0] — смещение по оси X (горизонтальное)
        #   flow[y, x, 1] — смещение по оси Y (вертикальное)
        try:
            flow = cv2.calcOpticalFlowFarneback(
                self.prev_gray, gray, None,
                pyr_scale=0.5,   # Масштаб пирамиды
                levels=3,        # Количество уровней
                winsize=15,      # Размер окна
                iterations=3,    # Итерации на уровне
                poly_n=5,        # Размер окна полинома
                poly_sigma=1.2,  # Сигма гауссиана
                flags=0          # Флаги
            )
        except cv2.error as e:
            print(f"[ОШИБКА] Ошибка вычисления оптического потока: {e}")
            self.prev_gray = gray.copy()  # Сбрасываем предыдущий кадр
            return None
        
        # === ШАГ 7: Оценка эго-движения камеры через гомографию (RANSAC) ===
        # Эго-движение — глобальное преобразование всего кадра из-за движения камеры.
        # RANSAC отбрасывает выбросы (движущиеся объекты) и находит чистое движение камеры.
        ego_motion_valid, expected_flow = self._estimate_ego_motion(flow)
        
        if ego_motion_valid:
            # Обновляем время последнего валидного движения
            self.last_move_time = current_time
            self.camera_moving = True
            
            # === ШАГ 8: Вычисление остаточного потока (параллакс) ===
            # Параллакс = разница между наблюдаемым потоком и ожидаемым от движения камеры
            # Близкие объекты имеют БОЛЬШИЙ параллакс (сильнее смещаются относительно фона)
            residual_flow_x = flow[..., 0] - expected_flow[..., 0]  # Разница по X
            residual_flow_y = flow[..., 1] - expected_flow[..., 1]  # Разница по Y
            
            # Вычисление величины вектора параллакса (евклидова норма)
            # np.hypot(a, b) = sqrt(a² + b²) — более стабильный вариант, чем ручной расчёт
            residual_magnitude = np.hypot(residual_flow_x, residual_flow_y)
            
            # === ШАГ 9: Преобразование параллакса в глубину ===
            # Физический принцип: параллакс ∝ 1/глубина
            #   Большой параллакс → малая глубина (объект близко)
            #   Малый параллакс → большая глубина (объект далеко)
            #
            # Формула: глубина = 1 / (параллакс + ε)
            #   ε = 0.5 — добавка для избежания деления на ноль и сглаживания шума
            current_depth = 1.0 / (residual_magnitude + 0.5)
            
            # === ШАГ 10: Внутреннее экспоненциальное сглаживание (для стабильности) ===
            # Не отображается напрямую, используется только для заполнения "дыр" при отсутствии движения
            if self.stable_depth is None:
                self.stable_depth = current_depth.copy()
            else:
                # Коэффициент сглаживания зависит от состояния движения:
                #   Движение (0.9) — быстрее реагируем на изменения
                #   Статика (0.99) — сильнее сглаживаем шум
                alpha = 0.9 if self.camera_moving else 0.99
                self.stable_depth = alpha * self.stable_depth + (1 - alpha) * current_depth
            
            # === ШАГ 11: Сглаживание ТЕКУЩЕЙ карты глубины с сохранением границ ===
            # cv2.bilateralFilter() — edge-aware фильтр, сохраняющий резкие переходы (границы объектов)
            # Параметры:
            #   src: входное изображение
            #   d=9: диаметр окрестности пикселей (9 пикселей)
            #   sigmaColor=75: фильтрация по цвету (больше → сильнее сглаживание)
            #   sigmaSpace=75: фильтрация по пространству (больше → сильнее сглаживание)
            depth_smoothed = cv2.bilateralFilter(current_depth, 9, 75, 75)
            
            # Обновляем предыдущий кадр для следующей итерации
            self.prev_gray = gray.copy()
            
            return depth_smoothed
        
        # === ШАГ 12: Обработка случая отсутствия валидного движения ===
        # Если камера недавно двигалась — возвращаем стабилизированную карту
        if self.camera_moving and self.stable_depth is not None:
            return cv2.bilateralFilter(self.stable_depth.copy(), 9, 75, 75)
        
        # Если нет движения и нет истории — возвращаем None
        self.prev_gray = gray.copy()
        return None
    
    def _estimate_ego_motion(self, flow):
        """
        Оценка глобального движения камеры (эго-движения) через гомографию с использованием RANSAC.
        
        Алгоритм:
        1. Создаём равномерную сетку точек по всему кадру
        2. Для каждой точки вычисляем ожидаемую позицию на основе оптического потока
        3. Находим гомографию (перспективное преобразование) между исходными и смещёнными точками
        4. RANSAC отбрасывает точки, не соответствующие глобальной модели (движущиеся объекты)
        5. Применяем гомографию ко всему кадру для получения "ожидаемого" потока
        
        Параметры:
        ----------
        flow : np.ndarray
            Оптический поток размером (H, W, 2)
        
        Возвращает:
        -----------
        (success, expected_flow) : tuple
            success : bool
                True, если гомография найдена с достаточным количеством inliers
            expected_flow : np.ndarray
                Ожидаемый поток для всего кадра на основе гомографии, размер (H, W, 2)
        """
        h, w = flow.shape[:2]
        
        # Проверка минимального размера кадра для корректной работы алгоритма
        if h < 50 or w < 50:
            return False, np.zeros_like(flow)
        
        # === СОЗДАНИЕ РАВНОМЕРНОЙ СЕТКИ ТОЧЕК ===
        # Шаг сетки: 16 пикселей (компромисс между точностью и скоростью)
        step = 16
        # np.mgrid[start:stop:step] создаёт координатную сетку
        # Пример: для изображения 480x640 с шагом 16 → ~1200 точек
        y_coords, x_coords = np.mgrid[step//2:h:step, step//2:w:step]
        # Преобразуем в массив точек формы (N, 2): [[x1, y1], [x2, y2], ...]
        points = np.vstack((x_coords.ravel(), y_coords.ravel())).T.astype(np.float32)
        
        if len(points) == 0:
            return False, np.zeros_like(flow)
        
        # === ВЫБОРКА ВЕКТОРОВ ПОТОКА В ТОЧКАХ СЕТКИ ===
        # Для каждой точки (x, y) берём вектор потока [dx, dy]
        try:
            # Индексация: flow[y, x] — сначала строка (y), потом столбец (x)
            flow_vectors = flow[y_coords.astype(int).ravel(), x_coords.astype(int).ravel()]
        except IndexError as e:
            print(f"[ОШИБКА] Индексация потока: {e}")
            return False, np.zeros_like(flow)
        
        # Вычисление ожидаемых позиций точек в следующем кадре
        next_points = points + flow_vectors  # Векторное сложение
        
        # Проверка минимального количества точек для RANSAC
        if len(points) < self.min_inliers * 2:
            return False, np.zeros_like(flow)
        
        # === ВЫЧИСЛЕНИЕ ГОМОГРАФИИ ЧЕРЕЗ RANSAC ===
        # cv2.findHomography() находит перспективное преобразование между двумя наборами точек
        # Параметры:
        #   srcPoints: исходные точки (в текущем кадре)
        #   dstPoints: целевые точки (в следующем кадре)
        #   method=cv2.RANSAC: метод устойчивой оценки с отбрасыванием выбросов
        #   ransacReprojThreshold=3.0: порог ошибки репроекции в пикселях (меньше → строже)
        #   maxIters=2000: максимальное количество итераций RANSAC
        #   confidence=0.995: доверительная вероятность нахождения правильной модели
        try:
            H, mask = cv2.findHomography(
                points, next_points,
                method=cv2.RANSAC,
                ransacReprojThreshold=3.0,
                maxIters=2000,
                confidence=0.995
            )
        except cv2.error as e:
            print(f"[ОШИБКА] Поиск гомографии: {e}")
            return False, np.zeros_like(flow)
        
        # Проверка результатов RANSAC
        if H is None or mask is None:
            return False, np.zeros_like(flow)
        
        inliers = np.sum(mask)  # Количество точек, соответствующих модели
        if inliers < self.min_inliers:
            # Недостаточно inliers — движение не валидно (возможно, только движущиеся объекты)
            return False, np.zeros_like(flow)
        
        # === ПРИМЕНЕНИЕ ГОМОГРАФИИ КО ВСЕМУ КАДРУ ===
        # Создаём координаты ВСЕХ пикселей кадра
        # Форма: (H*W, 1, 2) — требуется для cv2.perspectiveTransform
        coords = np.array([[x, y] for y in range(h) for x in range(w)], dtype=np.float32).reshape(-1, 1, 2)
        
        try:
            # Применяем гомографию: преобразуем координаты точек
            # Результат: новые координаты после перспективного преобразования
            warped_coords = cv2.perspectiveTransform(coords, H).reshape(-1, 2)
            
            # Вычисляем ожидаемый поток = разница между новыми и старыми координатами
            expected_flow_flat = warped_coords - coords.reshape(-1, 2)
            
            # Восстанавливаем исходную форму (H, W, 2)
            expected_flow = expected_flow_flat.reshape(h, w, 2)
            
            return True, expected_flow
            
        except Exception as e:
            print(f"[ОШИБКА] Применение гомографии: {e}")
            return False, np.zeros_like(flow)


def main():
    """
    Основная функция программы: цикл захвата кадров → обработка → отображение.
    
    Архитектура:
    1. Инициализация ресурсов (камера, оценщик глубины)
    2. Бесконечный цикл:
        a. Захват кадра с камеры
        b. Обработка кадра через OpticalFlowDepthEstimator → карта глубины
        c. Нормализация и визуализация карты глубины
        d. Отображение оригинала и карты глубины
        e. Обработка нажатия клавиш ('q' для выхода)
    3. Корректная очистка ресурсов
    """
    # === ИНИЦИАЛИЗАЦИЯ КАМЕРЫ ===
    print("[ИНФО] Инициализация веб-камеры...")
    cap = cv2.VideoCapture(0)
    
    if not cap.isOpened():
        raise IOError("[ОШИБКА] Не удалось подключиться к веб-камере (индекс 0). "
                      "Проверьте подключение и доступность камеры.")
    
    # Установка разрешения для стабильной работы алгоритма
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    actual_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    actual_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    print(f"[ИНФО] Камера инициализирована. Разрешение: {actual_width}x{actual_height}")
    
    # === ИНИЦИАЛИЗАЦИЯ ОЦЕНЩИКА ГЛУБИНЫ ===
    # Создаём объект оценщика. Загрузка происходит мгновенно (без тяжёлых моделей)
    estimator = OpticalFlowDepthEstimator(min_inliers=15, motion_timeout=1.5)
    
    # === ИНФОРМАЦИОННОЕ СООБЩЕНИЕ ПОЛЬЗОВАТЕЛЮ ===
    print("\n" + "="*70)
    print("  ПОСТРОЕНИЕ КАРТЫ ГЛУБИНЫ НА ОСНОВЕ ДВИЖЕНИЯ ОДНОЙ КАМЕРЫ")
    print("="*70)
    print("  Принцип работы:")
    print("    • Перемещайте камеру в пространстве (влево/вправо/вверх/вниз)")
    print("    • Алгоритм анализирует параллакс (разное смещение объектов)")
    print("    • Близкие объекты = большой параллакс = тёплые цвета (красный)")
    print("    • Дальние объекты = малый параллакс = холодные цвета (синий)")
    print("\n  Управление:")
    print("    • 'q' — выход из программы")
    print("    • Перемещайте камеру плавно для лучшего результата")
    print("="*70 + "\n")
    
    # === ГЛАВНЫЙ ЦИКЛ ОБРАБОТКИ ===
    frame_count = 0
    try:
        while True:
            frame_count += 1
            
            # Захват кадра с камеры
            ret, frame = cap.read()
            if not ret:
                print("[ОШИБКА] Не удалось получить кадр с камеры. Завершение работы.")
                break
            
            # === ОБРАБОТКА КАДРА: вызов оценщика глубины ===
            # Передаём кадр в оценщик. Внутри происходит:
            #   - конвертация в оттенки серого
            #   - вычисление оптического потока
            #   - оценка эго-движения через гомографию
            #   - выделение параллакса и преобразование в глубину
            depth_map = estimator(frame)
            
            # === ВИЗУАЛИЗАЦИЯ РЕЗУЛЬТАТОВ ===
            if depth_map is not None:
                # Нормализация карты глубины для отображения
                # Используем процентили для подавления выбросов (шум, артефакты)
                min_val = np.percentile(depth_map, 3)   # 3-й процентиль — отсекаем слишком тёмные пиксели
                max_val = np.percentile(depth_map, 97)  # 97-й процентиль — отсекаем слишком яркие пиксели
                
                # Линейная нормализация в диапазон [0, 1]
                if max_val > min_val:
                    depth_norm = np.clip((depth_map - min_val) / (max_val - min_val), 0, 1)
                else:
                    depth_norm = np.zeros_like(depth_map)
                
                # Конвертация в 8-битное изображение (0-255)
                depth_vis = (depth_norm * 255).astype(np.uint8)
                
                # Инверсия для правильной интерпретации цветовой карты:
                #   Близкие объекты должны быть ТЁПЛЫМИ (красный), а не холодными
                depth_inverted = 255 - depth_vis
                
                # Применение цветовой карты для наглядной визуализации
                # cv2.COLORMAP_TURBO — современная карта с плавными переходами и высокой контрастностью
                depth_colored = cv2.applyColorMap(depth_inverted, cv2.COLORMAP_TURBO)
                
                # Добавление индикатора состояния на изображения
                status_text = "ACTIVE (motion is detected)" if estimator.camera_moving else "STABLE (ожидание движения)"
                status_color = (0, 255, 0) if estimator.camera_moving else (0, 255, 255)  # Зелёный / Жёлтый
                
                # Текст на оригинальном кадре
                cv2.putText(
                    frame, status_text, (20, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, status_color, 2
                )
                
                # Текст на карте глубины
                cv2.putText(
                    depth_colored, "Deepth map", (20, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2
                )
                
                # Горизонтальное объединение изображений для отображения рядом
                combined = np.hstack((frame, depth_colored))
                cv2.imshow('Original | Deepth map (camera motion)', combined)
                
            else:
                # При отсутствии данных о глубине показываем только оригинальный кадр
                # с подсказкой пользователю
                cv2.putText(
                    frame, "MOVE CAMERA", (20, 40),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2  # Красный цвет
                )
                cv2.putText(
                    frame, "Waiting...", (20, 70),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1
                )
                cv2.imshow('Original | Deepth map (camera motion)', frame)
            
            # === ОБРАБОТКА НАЖАТИЙ КЛАВИШ ===
            # cv2.waitKey(1) — ожидание 1 мс с возможностью перехвата нажатия клавиши
            # & 0xFF — маска для кроссплатформенной совместимости (особенно 64-битные системы)
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                print(f"\n[ИНФО] Обработано кадров: {frame_count}. Выход по нажатию 'q'.")
                break
            
            # Дополнительные горячие клавиши (раскомментировать при необходимости):
            # elif key == ord('s'):
            #     # Сохранение текущей карты глубины
            #     if depth_map is not None:
            #         cv2.imwrite(f'depth_frame_{frame_count}.png', depth_colored)
            #         print(f"[ИНФО] Карта глубины сохранена: depth_frame_{frame_count}.png")
    
    finally:
        # === КОРРЕКТНАЯ ОЧИСТКА РЕСУРСОВ ===
        print("\n[ИНФО] Освобождение ресурсов...")
        cap.release()              # Освобождение камеры
        cv2.destroyAllWindows()    # Закрытие всех окон OpenCV
        print("[ИНФО] Работа программы завершена. Спасибо за использование!")


if __name__ == "__main__":
    """
    Точка входа в программу.
    
    Проверка __name__ == "__main__" гарантирует, что функция main() будет вызвана
    только при прямом запуске скрипта, а не при импорте как модуля.
    """
    main()