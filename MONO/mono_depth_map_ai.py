"""
https://pytorch.org/hub/intelisl_midas_v2/
"""


import torch
import cv2
import numpy as np


class DepthEstimator:
    """
    Класс для оценки карты глубины на основе предобученной модели MiDaS.
    
    Архитектура:
    - Инициализация (__init__): загрузка модели и трансформаций один раз при создании объекта
    - Вызов (__call__): обработка одного кадра → возврат цветной карты глубины
    
    Пример использования:
        estimator = DepthEstimator()
        depth_map = estimator(frame)  # frame — кадр от cv2.VideoCapture (BGR)
    """
    
    def __init__(self, model_type="MiDaS_small"):
        """
        Инициализация оценщика глубины.
        
        Параметры:
            model_type (str): тип модели MiDaS. 
                "MiDaS_small" — быстрая версия для реального времени (~20 FPS на GPU)
                "DPT_Large"   — точная, но медленная версия (~3 FPS на GPU)
        """
        # === ШАГ 1: Определение вычислительного устройства ===
        # Проверяем доступность CUDA (NVIDIA GPU). Если нет — используем CPU.
        # torch.cuda.is_available() возвращает True, если драйверы NVIDIA и CUDA установлены корректно.
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"[ИНФО] Используется устройство: {self.device}")
        
        # === ШАГ 2: Загрузка предобученной модели из репозитория PyTorch Hub ===
        # torch.hub.load(repo, model) автоматически скачивает модель из указанного репозитория.
        # Репозиторий "intel-isl/MiDaS" содержит официальные реализации моделей MiDaS.
        print(f"[ИНФО] Загрузка модели '{model_type}' из intel-isl/MiDaS...")
        self.model = torch.hub.load("intel-isl/MiDaS", model_type)
        
        # Перенос модели на выбранное устройство (GPU/CPU)
        self.model.to(self.device)
        
        # Перевод модели в режим оценки (inference mode):
        # - отключает dropout и другие слои, влияющие на обучение
        # - ускоряет вычисления и уменьшает потребление памяти
        self.model.eval()
        print("[ИНФО] Модель загружена и переведена в режим оценки (model.eval()).")
        
        # === ШАГ 3: Загрузка преобразований для входного изображения ===
        # Модель требует строго определённого формата входных данных:
        # - нормализация пикселей по определённым средним/стандартным отклонениям
        # - изменение размера до фиксированного разрешения (для small_transform — 256x256)
        # - перестановка осей из (H, W, C) в (C, H, W) — формат PyTorch
        transforms = torch.hub.load("intel-isl/MiDaS", "transforms")
        
        # Выбор трансформации в зависимости от типа модели:
        # - small_transform: упрощённая версия для MiDaS_small (быстрее, меньше требований к памяти)
        # - default_transform: стандартная версия для больших моделей (точнее, но медленнее)
        if model_type == "MiDaS_small":
            self.transform = transforms.small_transform
            print("[ИНФО] Выбрана упрощённая трансформация (small_transform) для быстрой обработки.")
        else:
            self.transform = transforms.default_transform
            print("[ИНФО] Выбрана стандартная трансформация (default_transform) для максимальной точности.")
    
    def __call__(self, frame):
        """
        Обработка одного кадра и возврат цветной карты глубины.
        
        Параметры:
            frame (np.ndarray): кадр от камеры в формате BGR (OpenCV), размер (H, W, 3)
        
        Возвращает:
            depth_colored (np.ndarray): цветная карта глубины в формате BGR, готовая к отображению через cv2.imshow()
        """
        # === ШАГ 1: Конвертация цветового пространства ===
        # Модель MiDaS обучена на изображениях в RGB, а OpenCV по умолчанию работает с BGR.
        # cv2.cvtColor(src, code) преобразует цветовое пространство:
        #   cv2.COLOR_BGR2RGB — меняет порядок каналов с BGR на RGB
        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # === ШАГ 2: Применение трансформаций к изображению ===
        # Трансформация возвращает тензор формы (1, C, H, W) — батч из одного изображения.
        # .to(self.device) переносит тензор на то же устройство, что и модель (GPU/CPU).
        input_batch = self.transform(img_rgb).to(self.device)
        # После трансформации: input_batch.shape = (1, 3, 256, 256) для small_transform
        
        # === ШАГ 3: Предсказание карты глубины моделью ===
        # torch.no_grad() — контекстный менеджер, отключающий вычисление градиентов.
        # Это критически важно для инференса: экономит память и ускоряет вычисления в 2-3 раза.
        with torch.no_grad():
            # Прямой проход через модель: получаем тензор предсказаний
            # Для MiDaS_small: prediction.shape = (1, H_out, W_out), где H_out=W_out=64 (уменьшенное разрешение)
            prediction = self.model(input_batch)
            
            # === ШАГ 4: Интерполяция к размеру исходного кадра ===
            # Модель возвращает карту глубины меньшего разрешения. Нужно увеличить её до размера кадра.
            # torch.nn.functional.interpolate() — функция изменения размера тензоров.
            # Параметры:
            #   input: тензор формы (N, C, H, W). prediction.unsqueeze(1) добавляет канал: (1, 1, 64, 64)
            #   size: целевой размер (высота, ширина) = размер кадра frame.shape[:2]
            #   mode="bicubic": метод интерполяции. "bicubic" даёт плавные переходы (лучше для глубины), 
            #                   альтернатива — "bilinear" (быстрее, но менее плавный результат)
            #   align_corners=False: математическая деталь интерполяции, для глубины лучше оставить False
            prediction = torch.nn.functional.interpolate(
                prediction.unsqueeze(1),          # Добавляем размерность канала: (1, 64, 64) → (1, 1, 64, 64)
                size=frame.shape[:2],             # Целевой размер = (высота кадра, ширина кадра)
                mode="bicubic",
                align_corners=False,
            ).squeeze()  # Убираем лишние размерности батча и канала: (1, 1, H, W) → (H, W)
            
            # Перенос тензора из GPU в CPU и конвертация в numpy массив
            prediction = prediction.cpu().numpy()
            # Теперь prediction.shape = (H, W) — двумерный массив значений глубины
        
        # === ШАГ 5: Нормализация значений глубины в диапазон [0, 255] ===
        # Значения prediction — произвольные float (например, от 10.0 до 1500.0).
        # cv2.normalize() приводит их к целочисленному диапазону 0-255 для отображения.
        # Параметры:
        #   src: исходный массив
        #   dst: None — результат будет возвращён, а не записан в dst
        #   alpha=0, beta=255: целевой диапазон [0, 255]
        #   norm_type=cv2.NORM_MINMAX: линейная нормализация: (x - min) / (max - min) * 255
        depth_map = cv2.normalize(prediction, None, 0, 255, cv2.NORM_MINMAX)
        
        # === ШАГ 6: Инверсия значений глубины (ближние объекты → тёмные) ===
        # По соглашению в компьютерном зрении:
        #   - малые значения = близкие объекты
        #   - большие значения = дальние объекты
        # После нормализации: близкие объекты = светлые (255), дальние = тёмные (0).
        # Инвертируем: 255 - value, чтобы близкие объекты стали тёмными.
        depth_map = 255 - depth_map.astype(np.uint8)  # Конвертируем во временный uint8 для вычитания
        
        # === ШАГ 7: Повторная инверсия для применения цветовой карты ===
        # Цветовые карты OpenCV (cv2.applyColorMap) интерпретируют:
        #   0 как "холодный" цвет (синий/фиолетовый)
        #   255 как "тёплый" цвет (красный/жёлтый)
        # Чтобы близкие объекты отображались тёплыми цветами (красный), а дальние — холодными (синий),
        # нужно ещё раз инвертировать перед применением цветовой карты.
        depth_map_inverted = 255 - depth_map  # Теперь: близкие = 255 (тёплые), дальние = 0 (холодные)
        
        # === ШАГ 8: Применение цветовой карты ===
        # cv2.applyColorMap(gray, colormap) преобразует одноканальное изображение в цветное.
        # Параметры:
        #   gray: одноканальное изображение (H, W) с значениями 0-255
        #   colormap: константа цветовой карты. 
        #       cv2.COLORMAP_TURBO — современная карта с плавными переходами (рекомендуется)
        #       Альтернативы: COLORMAP_JET (классическая, но с артефактами), COLORMAP_INFERNO, COLORMAP_MAGMA
        depth_colored = cv2.applyColorMap(depth_map_inverted, cv2.COLORMAP_TURBO)
        
        # === ШАГ 9: Возврат результата ===
        # Результат уже в формате BGR (как ожидает cv2.imshow), готов к отображению.
        return depth_colored


def main():
    """
    Основная функция программы: цикл захвата кадров → обработка → отображение.
    
    Логика работы:
    1. Инициализация ресурсов (модель глубины, камера)
    2. Бесконечный цикл:
        a. Захват кадра с камеры
        b. Обработка кадра через DepthEstimator → получение карты глубины
        c. Отображение оригинала и карты глубины
        d. Проверка нажатия клавиши 'q' для выхода
    3. Корректная очистка ресурсов при завершении
    """
    # === ИНИЦИАЛИЗАЦИЯ ===
    # Создаём объект оценщика глубины. Происходит загрузка модели (занимает 5-15 секунд в первый раз).
    estimator = DepthEstimator()
    
    # Инициализация захвата видео с веб-камеры.
    # cv2.VideoCapture(0) — камера по умолчанию (обычно встроенная в ноутбук).
    # Для внешней камеры может потребоваться индекс 1, 2 и т.д.
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        raise IOError("Ошибка: не удалось подключиться к веб-камере (индекс 0). Проверьте подключение камеры.")
    print("[ИНФО] Камера успешно инициализирована. Разрешение кадра: {}x{}".format(
        int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
        int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    ))
    print("[ИНФО] Управление: нажмите клавишу 'q' для выхода из программы.")
    
    # === ГЛАВНЫЙ ЦИКЛ ОБРАБОТКИ ===
    try:
        frame_count = 0
        while True:
            frame_count += 1
            
            # Захват одного кадра из видеопотока.
            # ret — булево значение: True, если кадр успешно захвачен.
            # frame — numpy массив формы (H, W, 3) в формате BGR.
            ret, frame = cap.read()
            if not ret:
                print("[ОШИБКА] Не удалось получить кадр с камеры. Возможно, камера отключена или занята другой программой.")
                break
            
            # === ОБРАБОТКА КАДРА: вызов оценщика глубины ===
            # Передаём кадр в оценщик. Внутри происходит:
            #   - конвертация цвета
            #   - применение трансформаций
            #   - предсказание моделью
            #   - интерполяция и постобработка
            # Возвращается цветная карта глубины (BGR) для отображения.
            depth_colored = estimator(frame)
            
            # === ОТОБРАЖЕНИЕ РЕЗУЛЬТАТОВ ===
            # cv2.imshow(window_name, image) — отображает изображение в отдельном окне.
            # Окна создаются автоматически при первом вызове с данным именем.
            cv2.imshow('Original', frame)          # Окно с исходным кадром с камеры
            cv2.imshow('Deepth map', depth_colored)  # Окно с цветной картой глубины
            
            # === ОБРАБОТКА НАЖАТИЙ КЛАВИШ ===
            # cv2.waitKey(delay) — ждёт нажатия клавиши заданное время (в миллисекундах).
            #   delay=1 — ждать 1 мс (практически немедленный возврат, но позволяет обработать события окна).
            #   Возвращает код нажатой клавиши или -1, если ничего не нажато.
            #   & 0xFF — маска для получения младшего байта (важно для 64-битных систем).
            #   ord('q') — ASCII-код символа 'q' (113).
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                print(f"[ИНФО] Обработано кадров: {frame_count}. Выход по нажатию клавиши 'q'.")
                break

            elif key == ord('s'):
                 cv2.imwrite(f'depth_{frame_count}.png', depth_colored)
                 print(f"[ИНФО] Карта глубины сохранена как depth_{frame_count}.png")
                
    finally:
        # === ОЧИСТКА РЕСУРСОВ (гарантированно выполняется даже при исключении) ===
        print("[ИНФО] Освобождение ресурсов...")
        
        # Освобождение камеры: закрытие видеопотока, освобождение драйвера камеры.
        cap.release()
        
        # Закрытие всех окон OpenCV, созданных через cv2.imshow().
        cv2.destroyAllWindows()
        
        print("[ИНФО] Работа программы завершена. Все ресурсы освобождены.")


if __name__ == "__main__":
    """
    Точка входа в программу.
    Проверка __name__ == "__main__" гарантирует, что main() запустится только при прямом запуске скрипта,
    а не при импорте этого файла как модуля в другую программу.
    """
    main()